{"cells":[{"cell_type":"code","source":["print(\"\"\"\nclass TrueLift:\n  def __init__(self, df, id_label, control_label, \n               target_value_label, prediction_value_label):\n    (spark_df, str, str, str, str) -> None\n    Input dataframe must contain 4 columns:\n    label for row_id, control/treatment, target_value, and prediction_value.\n    \n  def report_grad(self, repartition=True, N=10):\n    (bool, int) -> int, spark_df\n    Calculate gradient and the minimum subset size.\n\n  def update_pred(self, new_pred_df, new_pred_label):\n    (spark_df, str) -> None\n    Update self.df[self.pred_label] to the values from the input table.\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import pyspark.sql.functions as F\nfrom pyspark.sql.types import *\nimport numpy as np\nfrom pyspark.ml.feature import QuantileDiscretizer"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["class TrueLift:\n  def __init__(self, df, id_label, control_label, \n               target_value_label, prediction_value_label):\n    \"\"\"\n    (spark_df, str, str, str, str)\n    Input dataframe must contain 4 columns:\n    label for row_id, control/treatment, target_value, and prediction_value.\n    \"\"\"\n    self.df = df.select(id_label, control_label, target_value_label, prediction_value_label)\n    self.df.persist()\n    self.id_label = id_label\n    self.control_label = control_label\n    self.target_value_label = target_value_label\n    self.pred_label = prediction_value_label\n    self.global_stats_df = df.groupby(self.control_label\n                            ).agg(F.mean(self.target_value_label\n                                  ).alias(\"ave_target_value\")\n                                 )\n    control_value = self.global_stats_df.filter(F.col(self.control_label)==1\n                                       ).select(\"ave_target_value\"\n                                       ).collect()[0][0]\n    treatment_value = self.global_stats_df.filter(F.col(self.control_label)==0\n                                         ).select(\"ave_target_value\"\n                                         ).collect()[0][0]\n    self.global_lift = treatment_value-control_value\n    self.data_size = df.count()\n    self.treatment_size = df.filter(F.col(self.control_label)==0).count()\n    self.control_size = df.filter(F.col(self.control_label)==1).count()\n    return None\n  \n  def get_cuts(self, N=10):\n    \"\"\"\n    Get the threshold values which can split the data \n    into N subsets based on the prediction value.\n    \"\"\"\n    self.subset_count = N\n    self.df = self.df.drop(\"subset\")\n    self.qt = QuantileDiscretizer(numBuckets=N, \n                                  inputCol=self.pred_label, \n                                  outputCol=\"subset\").fit(self.df)\n    self.subset_cuts = self.qt.getSplits()\n    return None\n  \n  def split_subsets(self):\n    \"\"\"\n    Assign subset values to the table.\n    \"\"\"\n    self.df = self.df.drop(\"subset\")\n    self.df = self.qt.transform(self.df)\n    self.df = self.df.withColumn(\"subset\", F.col(\"subset\").cast(IntegerType())+1)\n    return None\n  \n  def get_subset_stats(self):\n    \"\"\"\n    Get average values within each subset.\n    \"\"\"\n    total_size_df = self.df.groupby(\"subset\"\n                              ).agg(F.count(self.pred_label).alias(\"subset_size\"),\n                                    F.mean(self.pred_label).alias(\"ave_pred_value\"))\n    control_stats_df = self.df.filter(F.col(self.control_label)==1\n                             ).groupby(\"subset\"\n                             ).agg(F.count(self.pred_label).alias(\"control_subset_size\"),\n                                   F.mean(self.target_value_label).alias(\"ave_control_target_value\"))\n    treatment_stats_df = self.df.filter(F.col(self.control_label)==0\n                               ).groupby(\"subset\"\n                               ).agg(F.count(self.pred_label).alias(\"treatment_subset_size\"),\n                                     F.mean(self.target_value_label).alias(\"ave_treatment_target_value\"))\n    self.subset_stats_df = total_size_df.join(control_stats_df, [\"subset\"], how=\"left\"\n                                       ).join(treatment_stats_df, [\"subset\"], how=\"left\")\n    self.subset_stats_df = self.subset_stats_df.withColumn(\"ave_lift\", \n                                                           F.col(\"ave_treatment_target_value\")\n                                                          -F.col(\"ave_control_target_value\"))\n    min_control_subset_size = self.subset_stats_df.groupby(\n                                 ).agg(F.min(\"control_subset_size\")).collect()[0][0]\n    min_treatment_subset_size = self.subset_stats_df.groupby(\n                                   ).agg(F.min(\"treatment_subset_size\")).collect()[0][0]\n    return min(min_control_subset_size, min_treatment_subset_size)\n  \n  def loss_function(self):\n    \"\"\"\n    () -> float\n    Output the value of the loss function.\n    Base on current subset assignments.\n    \"\"\"\n    self.subset_stats_df = self.subset_stats_df.withColumn(\"Loss\", \n                                                           (F.col(\"subset_size\")/self.data_size)\n                                                           *( (F.col(\"ave_pred_value\")-F.col(\"ave_lift\"))**2\n                                                             -(F.col(\"ave_lift\")-self.global_lift)**2\n                                                            )\n                                                          )\n    loss = self.subset_stats_df.groupby().sum(\"Loss\").collect()[0][0]\n    return loss\n\n  def get_tri_segment_cuts(self):\n    \"\"\"\n    Calculate the threshold values that can split each \n    subset into 3 segments.\n    \"\"\"\n    self.upper_cuts = list( (1/3)*np.array([c for c in self.subset_cuts[1:-1]])\n                           +(2/3)*np.array([c for c in self.subset_cuts[2:]]) )\n    self.lower_cuts = list( (2/3)*np.array([c for c in self.subset_cuts[0:-2]])\n                           +(1/3)*np.array([c for c in self.subset_cuts[1:-1]]) )\n    self.upper_cuts = [self.subset_cuts[1]-(self.lower_cuts[1]-self.subset_cuts[1])] + self.upper_cuts\n    self.lower_cuts = self.lower_cuts + [self.subset_cuts[-2]+(self.subset_cuts[-2]-self.upper_cuts[-2])]\n    return None\n  \n  def assign_segments(self):\n    \"\"\"\n    Attach the segment values to the table.\n    \"\"\"\n    upper_cuts = [float(x) for x in self.upper_cuts]\n    lower_cuts = [float(x) for x in self.lower_cuts]\n    get_upper_cuts = F.udf(lambda x: upper_cuts[x-1], FloatType())\n    get_lower_cuts = F.udf(lambda x: lower_cuts[x-1], FloatType())\n    self.df = self.df.withColumn(\"Upper_Cut\", get_upper_cuts(\"subset\"))\n    self.df = self.df.withColumn(\"Lower_Cut\", get_lower_cuts(\"subset\"))\n    self.df = self.df.withColumn(\"Segment\", F.when(F.col(\"Pred\")>F.col(\"Upper_Cut\"), \"Top\"\n                                            ).when(F.col(\"Pred\")<F.col(\"Lower_Cut\"), \"Bot\"\n                                            ).otherwise(\"Mid\") \n                                )\n    self.df = self.df.drop(\"Upper_Cut\", \"Lower_Cut\")\n    return None\n  \n  def get_grad_lookup(self):\n    \"\"\"\n    Obtain all parameters required to calculate the gradient\n    whcih are constant within each subset.\n    \"\"\"\n    # Attach all cut-value information into the stats table.\n    upper_cuts = [float(x) for x in self.upper_cuts]\n    lower_cuts = [float(x) for x in self.lower_cuts]\n    subset_cuts = [float(x) for x in self.subset_cuts]\n    get_upper_cuts = F.udf(lambda x: upper_cuts[x-1], FloatType())\n    get_lower_cuts = F.udf(lambda x: lower_cuts[x-1], FloatType()) \n    get_upper_bounds = F.udf(lambda x: subset_cuts[x], FloatType())\n    get_lower_bounds = F.udf(lambda x: subset_cuts[x-1], FloatType())\n    self.grad_lookup_df = self.subset_stats_df.withColumn(\"Upper_Cut\", get_upper_cuts(\"subset\"))\n    self.grad_lookup_df = self.grad_lookup_df.withColumn(\"Lower_Cut\", get_lower_cuts(\"subset\"))\n    self.grad_lookup_df = self.grad_lookup_df.withColumn(\"Upper_Bound\", get_upper_bounds(\"subset\"))\n    self.grad_lookup_df = self.grad_lookup_df.withColumn(\"Lower_Bound\", get_lower_bounds(\"subset\"))\n    # Rename some columns to match the paper's convention.\n    self.grad_lookup_df = self.grad_lookup_df.withColumnRenamed(\"Subset_Size\", \"S_n\")\n    self.grad_lookup_df = self.grad_lookup_df.withColumnRenamed(\"ave_pred_value\", \"P_n\")\n    self.grad_lookup_df = self.grad_lookup_df.withColumnRenamed(\"control_subset_size\", \"SC_n\")\n    self.grad_lookup_df = self.grad_lookup_df.withColumnRenamed(\"treatment_subset_size\", \"ST_n\")\n    self.grad_lookup_df = self.grad_lookup_df.withColumnRenamed(\"ave_lift\", \"lbar_n\")\n    self.grad_lookup_df = self.grad_lookup_df.withColumnRenamed(\"ave_control_target_value\", \"ybarC_n\")\n    self.grad_lookup_df = self.grad_lookup_df.withColumnRenamed(\"ave_treatment_target_value\", \"ybarT_n\")\n    # Calculate the parameters in the gradient which are the same within a subset.\n    self.grad_lookup_df = self.grad_lookup_df.withColumn(\"dLdp_bias\", \n                                                         2*(F.col(\"P_n\")-F.col(\"lbar_n\"))\n                                                         /self.data_size\n                                                        )                                                        \n    self.grad_lookup_df = self.grad_lookup_df.withColumn(\"dp_top\", \n                                                         (F.col(\"Upper_Bound\")-F.col(\"Upper_Cut\"))/2\n                                                        )\n    self.grad_lookup_df = self.grad_lookup_df.withColumn(\"dp_bot\", \n                                                         (F.col(\"Lower_Bound\")-F.col(\"Lower_Cut\"))/2\n                                                        )\n    self.grad_lookup_df = self.grad_lookup_df.withColumn(\"dLdlbar_n\", \n                                                         (-2*(F.col(\"P_n\")-F.col(\"lbar_n\"))\n                                                          -2*(F.col(\"lbar_n\")-self.global_lift) )\n                                                         *(F.col(\"S_n\")/self.data_size)\n                                                        )\n    self.grad_lookup_df = self.grad_lookup_df.withColumn(\"dLdS_n\", \n                                                         ( (F.col(\"P_n\")-F.col(\"lbar_n\"))**2\n                                                          -(F.col(\"lbar_n\")-self.global_lift)**2 )\n                                                         /self.data_size\n                                                        )\n    self.grad_lookup_df = self.grad_lookup_df.select(\"subset\", \"dLdp_bias\", \"dp_top\", \"dp_bot\",\n                                                     \"SC_n\", \"ST_n\", \"ybarC_n\", \"ybarT_n\",\n                                                     \"dLdlbar_n\", \"dLdS_n\" )\n    shift_columns = [\"SC_n\", \"ST_n\", \"ybarC_n\", \"ybarT_n\",\n                     \"dLdlbar_n\", \"dLdS_n\"]\n    np1_df =self.grad_lookup_df.select( (F.col(\"subset\")-1).alias(\"subset\"),\n                                       *[F.col(c).alias(c[:-1]+\"(n+1)\") for c in shift_columns])\n    nm1_df =self.grad_lookup_df.select( (F.col(\"subset\")+1).alias(\"subset\"),\n                                       *[F.col(c).alias(c[:-1]+\"(n-1)\") for c in shift_columns])\n    self.grad_lookup_df = self.grad_lookup_df.join(np1_df, [\"subset\"], how=\"left\")\n    self.grad_lookup_df = self.grad_lookup_df.join(nm1_df, [\"subset\"], how=\"left\")\n    return None\n  \n  def calc_grad(self):\n    columns_to_keep = self.df.columns[:]\n    df = self.df.join(self.grad_lookup_df, [\"subset\"], how=\"left\")\n    df = df.withColumn(\"gradient\", \n                        F.when(F.col(\"Segment\")==\"Mid\", F.col(\"dLdp_bias\")\n                        ).when( (F.col(\"Segment\")==\"Top\") \n                                &(F.col(\"Control_Flag\")==0), \n                               F.col(\"dLdp_bias\")\n                               +(1/F.col(\"dp_top\"))\n                                *( F.col(\"dLdlbar_n\")\n                                   *((-F.col(\"Target_Value\")+F.col(\"ybarT_n\"))\n                                     /F.col(\"ST_n\")\n                                    )\n                                  +F.col(\"dLdlbar_(n+1)\")\n                                   *((F.col(\"Target_Value\")-F.col(\"ybarT_(n+1)\"))\n                                     /F.col(\"ST_(n+1)\")\n                                    )\n                                  -F.col(\"dLdS_n\")+F.col(\"dLdS_(n+1)\")\n                                 )\n                        ).when( (F.col(\"Segment\")==\"Top\") \n                                &(F.col(\"Control_Flag\")==1), \n                               F.col(\"dLdp_bias\")\n                               +(1/F.col(\"dp_top\"))\n                                *( F.col(\"dLdlbar_n\")\n                                   *((F.col(\"Target_Value\")-F.col(\"ybarC_n\"))\n                                     /F.col(\"SC_n\")\n                                    )\n                                  +F.col(\"dLdlbar_(n+1)\")\n                                   *((-F.col(\"Target_Value\")+F.col(\"ybarC_(n+1)\"))\n                                     /F.col(\"SC_(n+1)\")\n                                    )\n                                  -F.col(\"dLdS_n\")+F.col(\"dLdS_(n+1)\")\n                                 )\n                        ).when( (F.col(\"Segment\")==\"Bot\") \n                                &(F.col(\"Control_Flag\")==0), \n                               F.col(\"dLdp_bias\")\n                               +(1/F.col(\"dp_bot\"))\n                                *( F.col(\"dLdlbar_n\")\n                                   *((-F.col(\"Target_Value\")+F.col(\"ybarT_n\"))\n                                     /F.col(\"ST_n\")\n                                    )\n                                  +F.col(\"dLdlbar_(n-1)\")\n                                   *((F.col(\"Target_Value\")-F.col(\"ybarT_(n-1)\"))\n                                     /F.col(\"ST_(n-1)\")\n                                    )\n                                  -F.col(\"dLdS_n\")+F.col(\"dLdS_(n-1)\")\n                                 )\n                        ).otherwise(\n                               F.col(\"dLdp_bias\")\n                               +(1/F.col(\"dp_bot\"))\n                                *( F.col(\"dLdlbar_n\")\n                                   *((F.col(\"Target_Value\")-F.col(\"ybarC_n\"))\n                                     /F.col(\"SC_n\")\n                                    )\n                                  +F.col(\"dLdlbar_(n-1)\")\n                                   *((-F.col(\"Target_Value\")+F.col(\"ybarC_(n-1)\"))\n                                     /F.col(\"SC_(n-1)\")\n                                    )\n                                  -F.col(\"dLdS_n\")+F.col(\"dLdS_(n-1)\")\n                                 )\n                        )\n                       )\n    return df.select(self.id_label, \"gradient\")\n  \n  def report_grad(self, repartition=True, N=10):\n    \"\"\"\n    (bool, int) -> int, spark_df\n    Calculate gradient and the minimum subset size.\n    \"\"\"\n    if repartition:\n      self.get_cuts(N)\n      self.split_subsets()\n      min_size = self.get_subset_stats()\n      self.get_tri_segment_cuts()\n      self.assign_segments()\n      self.get_grad_lookup()\n      grad = self.calc_grad()\n    else:\n      self.split_subsets()\n      min_size = self.get_subset_stats()\n      self.assign_segments()\n      self.get_grad_lookup()\n      grad = self.calc_grad()\n    return min_size, grad\n  \n  def update_pred(self, new_pred_df, new_pred_label):\n    \"\"\"\n    (spark_df, str) -> None\n    Update self.df[self.pred_label] to the values from the input table.\n    \"\"\"\n    self.df = self.df.join(new_pred_df, [self.id_label], how=\"left\")\n    self.df = self.df.withColumn(self.pred_label, F.col(new_pred_label))\n    self.df = self.df.drop(new_pred_label)\n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3}],"metadata":{"name":"class_loss_function","notebookId":2973226778536516},"nbformat":4,"nbformat_minor":0}
